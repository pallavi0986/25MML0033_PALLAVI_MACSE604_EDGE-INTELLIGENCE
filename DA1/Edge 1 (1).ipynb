{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7f3e523-ba4e-48aa-a6f4-5fb8b067cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pallavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a658a6e-a134-4b45-8f25-1cc631009f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text loaded successfully. Sample:\n",
      "﻿The Project Gutenberg eBook of Walled towns\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/77444/pg77444.txt\"\n",
    "response = requests.get(url)\n",
    "text_data = response.text\n",
    "print(\"Text loaded successfully. Sample:\")\n",
    "print(text_data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1369a7f-e67a-4d90-a5a6-7e2c2d318c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pallavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pallavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pallavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2db7a73d-d96c-4709-b184-2d092df45c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences found: 648\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text_data)\n",
    "print(f\"\\nTotal sentences found: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ac0df8d-6cb3-456a-988e-a8a4a6fc622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokenization:\n",
      "[['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Walled', 'towns', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.'], ['You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '.']]\n",
      "\n",
      "Sample cleaned tokens: ['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'of', 'walled', 'towns', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(\"\\nSample tokenization:\")\n",
    "print(tokenized_sentences[:2])\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "clean_tokens = []\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in punct:\n",
    "            clean_tokens.append(word)\n",
    "\n",
    "print(\"\\nSample cleaned tokens:\", clean_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd5e421d-8768-4ec4-889c-2218e4e805d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after removing stopwords: ['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'walled', 'towns', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'states', 'parts', 'world', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens_no_stop = [word for word in clean_tokens if word not in stop_words]\n",
    "print(\"\\nTokens after removing stopwords:\", tokens_no_stop[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "092bab99-b1c5-4930-9750-763e7e5496fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized sample: ['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'walled', 'town', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'state', 'part', 'world', 'cost', 'almost', 'restriction', 'whatsoever', 'may', 'copy']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stop]\n",
    "print(\"\\nLemmatized sample:\", lemmatized_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af23e4b6-6291-425a-9bf8-c8276410e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed sample: ['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'wall', 'town', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "print(\"\\nStemmed sample:\", stemmed_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af70866b-d52c-4f5f-96e9-25696985b7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Common Words:\n",
      "[('“', 113), ('town', 109), ('”', 108), ('work', 91), ('project', 88), ('may', 75), ('one', 71), ('life', 63), ('walled', 60), ('gutenberg™', 57), ('must', 54), ('would', 51), ('great', 42), ('--', 42), ('state', 40), ('world', 40), ('even', 40), ('year', 40), ('new', 39), ('thing', 38)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist = FreqDist(lemmatized_tokens)\n",
    "\n",
    "print(\"\\nMost Common Words:\")\n",
    "print(freq_dist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a632e-53e7-4877-a627-1ab5cf472cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
